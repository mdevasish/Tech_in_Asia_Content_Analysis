{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-3a4a0d506139>:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm_notebook().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82920156a69a4d6a96bb9c046ed0a157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize,word_tokenize,pos_tag\n",
    "import spacy\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import datetime\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "my_stopwords = stopwords.words('English') + ['photo','credit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data/posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n['  Every day, 100k+ smart people read our newsletter.',\\n 'You can  sign up here .',\\n 'Hello readers, \\n Two familiar names were spotted making investments this week: Tencent and Alibaba.']\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentences'] = data.content.apply(sent_tokenize)\n",
    "\n",
    "data['sentences'].head(1).tolist()[0][:3]\n",
    "\n",
    "'''\n",
    "['  Every day, 100k+ smart people read our newsletter.',\n",
    " 'You can  sign up here .',\n",
    " 'Hello readers, \\n Two familiar names were spotted making investments this week: Tencent and Alibaba.']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35076a087c64a81a6a14460c6804efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=900.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['Every', 'day', ',', '100k+', 'smart', 'people', 'read', 'our', 'newsletter', '.'], ['You', 'can', 'sign', 'up', 'here', '.'], ['Hello', 'readers', ',', 'Two', 'familiar', 'names', 'were', 'spotted', 'making', 'investments', 'this', 'week', ':', 'Tencent', 'and', 'Alibaba', '.']]\n"
     ]
    }
   ],
   "source": [
    "data['tokens_sentences'] = data['sentences'].progress_map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "print(data['tokens_sentences'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Every', 'day', ',', '100k+', 'smart', 'people', 'read', 'our', 'newsletter', '.'], ['You', 'can', 'sign', 'up', 'here', '.'], ['Hello', 'readers', ',', 'Two', 'familiar', 'names', 'were', 'spotted', 'making', 'investments', 'this', 'week', ':', 'Tencent', 'and', 'Alibaba', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(data['tokens_sentences'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270cb015d06a431cab6c06ab9fc66fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=900.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[('Every', 'DT'), ('day', 'NN'), (',', ','), ('100k+', 'CD'), ('smart', 'JJ'), ('people', 'NNS'), ('read', 'VBP'), ('our', 'PRP$'), ('newsletter', 'NN'), ('.', '.')], [('You', 'PRP'), ('can', 'MD'), ('sign', 'VB'), ('up', 'RP'), ('here', 'RB'), ('.', '.')], [('Hello', 'NNP'), ('readers', 'NNS'), (',', ','), ('Two', 'CD'), ('familiar', 'JJ'), ('names', 'NNS'), ('were', 'VBD'), ('spotted', 'VBN'), ('making', 'VBG'), ('investments', 'NNS'), ('this', 'DT'), ('week', 'NN'), (':', ':'), ('Tencent', 'NN'), ('and', 'CC'), ('Alibaba', 'NNP'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "data['POS_tokens'] = data['tokens_sentences'].progress_map(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "print(data['POS_tokens'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd266f29e9c48879ce65511f666fb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=900.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['tokens_sentences_lemmatized'] = data['POS_tokens'].progress_map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Every',\n",
       "  'day',\n",
       "  ',',\n",
       "  '100k+',\n",
       "  'smart',\n",
       "  'people',\n",
       "  'read',\n",
       "  'our',\n",
       "  'newsletter',\n",
       "  '.'],\n",
       " ['You', 'can', 'sign', 'up', 'here', '.'],\n",
       " ['Hello',\n",
       "  'reader',\n",
       "  ',',\n",
       "  'Two',\n",
       "  'familiar',\n",
       "  'name',\n",
       "  'be',\n",
       "  'spot',\n",
       "  'make',\n",
       "  'investment',\n",
       "  'this',\n",
       "  'week',\n",
       "  ':',\n",
       "  'Tencent',\n",
       "  'and',\n",
       "  'Alibaba',\n",
       "  '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens_sentences_lemmatized'].head(1).tolist()[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "data['tokens'] = data['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every',\n",
       " 'day',\n",
       " 'smart',\n",
       " 'people',\n",
       " 'read',\n",
       " 'newsletter',\n",
       " 'sign',\n",
       " 'hello',\n",
       " 'reader',\n",
       " 'two',\n",
       " 'familiar',\n",
       " 'name',\n",
       " 'spot',\n",
       " 'make',\n",
       " 'investment',\n",
       " 'week',\n",
       " 'tencent',\n",
       " 'alibaba',\n",
       " 'intense',\n",
       " 'rivalry',\n",
       " 'chinese',\n",
       " 'giant',\n",
       " 'die',\n",
       " 'anytime',\n",
       " 'soon',\n",
       " 'week',\n",
       " 'two',\n",
       " 'unknowingly',\n",
       " 'bring',\n",
       " 'clash']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'].head(1).tolist()[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data['tokens'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = gensim.corpora.Dictionary(tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123456)\n",
    "num_topics = 20\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 15:53:24.213222\n",
      "#Topics: 2 Coherence Score:  0.24925239320766918 Perplexity score : -8.508362460276315\n",
      "#Topics: 3 Coherence Score:  0.25794003730491794 Perplexity score : -8.624460907788004\n",
      "#Topics: 4 Coherence Score:  0.23564634553708685 Perplexity score : -8.734755334518242\n",
      "#Topics: 5 Coherence Score:  0.2490754466003894 Perplexity score : -8.796507822619725\n",
      "#Topics: 6 Coherence Score:  0.2540845448986197 Perplexity score : -8.858655273513795\n",
      "#Topics: 7 Coherence Score:  0.2455929439936954 Perplexity score : -8.948534697869487\n",
      "#Topics: 8 Coherence Score:  0.2445575689123854 Perplexity score : -9.023336902832995\n",
      "#Topics: 9 Coherence Score:  0.26292386484893165 Perplexity score : -9.05561824789073\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "    \n",
    "model_list = []\n",
    "coherence_values = []\n",
    "perplexity_values = []\n",
    "model_topics = []\n",
    "\n",
    "for num_topics in range(2,10):\n",
    "    lda_x = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "    coherencemodel = CoherenceModel(model=lda_x, texts=tokens, dictionary=dictionary_LDA, coherence='c_v')\n",
    "    model_topics.append(num_topics)\n",
    "    model_list.append(lda_x)\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    perplexity_values.append(lda_x.log_perplexity(corpus))\n",
    "    print(\"#Topics: \" + str(num_topics) + \" Coherence Score: \" \n",
    "              , str(coherencemodel.get_coherence())+ ' Perplexity score : '+ str(lda_x.log_perplexity(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.010*\"gudangada\" + 0.008*\"market\" + 0.008*\"user\" + 0.008*\"app\" + 0.007*\"agent\" + 0.007*\"product\" + 0.006*\"firm\" + 0.006*\"new\" + 0.005*\"like\" + 0.005*\"china\" + 0.005*\"apps\" + 0.005*\"use\" + 0.005*\"invest\" + 0.005*\"super\" + 0.004*\"platform\" + 0.004*\"indonesia\" + 0.004*\"people\" + 0.004*\"social_commerce_startup\" + 0.003*\"however\" + 0.003*\"offer\"\n",
      "\n",
      "1: 0.009*\"gojek\" + 0.007*\"product\" + 0.007*\"tiktok\" + 0.007*\"market\" + 0.006*\"india\" + 0.005*\"country\" + 0.005*\"indonesia\" + 0.005*\"include\" + 0.004*\"make\" + 0.004*\"photo_credit\" + 0.004*\"help\" + 0.004*\"platform\" + 0.004*\"zenius\" + 0.004*\"investment\" + 0.004*\"service\" + 0.004*\"ceo\" + 0.004*\"new\" + 0.004*\"user\" + 0.004*\"year\" + 0.003*\"work\"\n",
      "\n",
      "2: 0.008*\"market\" + 0.005*\"online\" + 0.005*\"platform\" + 0.005*\"user\" + 0.005*\"new\" + 0.004*\"one\" + 0.004*\"service\" + 0.004*\"include\" + 0.004*\"firm\" + 0.004*\"southeast_asia\" + 0.004*\"make\" + 0.004*\"product\" + 0.004*\"like\" + 0.003*\"content\" + 0.003*\"however\" + 0.003*\"use\" + 0.003*\"india\" + 0.003*\"china\" + 0.003*\"go\" + 0.003*\"country\"\n",
      "\n",
      "3: 0.006*\"may\" + 0.006*\"mitra_bukalapak\" + 0.005*\"country\" + 0.005*\"investment\" + 0.005*\"product\" + 0.005*\"new\" + 0.004*\"include\" + 0.004*\"fund\" + 0.004*\"us\" + 0.004*\"photo_credit\" + 0.004*\"well\" + 0.004*\"team\" + 0.003*\"pay\" + 0.003*\"management\" + 0.003*\"help\" + 0.003*\"market\" + 0.003*\"softbank\" + 0.003*\"warungs\" + 0.003*\"add\" + 0.003*\"would\"\n",
      "\n",
      "4: 0.009*\"one\" + 0.007*\"country\" + 0.006*\"make\" + 0.006*\"employee\" + 0.005*\"market\" + 0.005*\"get\" + 0.005*\"indonesia\" + 0.005*\"platform\" + 0.005*\"include\" + 0.004*\"people\" + 0.004*\"investment\" + 0.004*\"us\" + 0.004*\"investor\" + 0.004*\"team\" + 0.004*\"even\" + 0.004*\"user\" + 0.003*\"work\" + 0.003*\"share\" + 0.003*\"gojek\" + 0.003*\"help\"\n",
      "\n",
      "5: 0.007*\"make\" + 0.006*\"one\" + 0.005*\"need\" + 0.005*\"even\" + 0.005*\"use\" + 0.004*\"market\" + 0.004*\"work\" + 0.004*\"would\" + 0.004*\"get\" + 0.004*\"like\" + 0.004*\"time\" + 0.004*\"help\" + 0.004*\"product\" + 0.004*\"us\" + 0.004*\"tech\" + 0.004*\"go\" + 0.004*\"well\" + 0.004*\"people\" + 0.004*\"take\" + 0.004*\"new\"\n",
      "\n",
      "6: 0.008*\"india\" + 0.007*\"platform\" + 0.005*\"product\" + 0.005*\"us\" + 0.005*\"service\" + 0.005*\"country\" + 0.005*\"employee\" + 0.004*\"market\" + 0.004*\"year\" + 0.004*\"new\" + 0.004*\"oyo\" + 0.004*\"help\" + 0.004*\"make\" + 0.003*\"include\" + 0.003*\"offer\" + 0.003*\"add\" + 0.003*\"come\" + 0.003*\"use\" + 0.003*\"well\" + 0.003*\"see\"\n",
      "\n",
      "7: 0.006*\"customer\" + 0.006*\"market\" + 0.005*\"new\" + 0.005*\"us\" + 0.005*\"user\" + 0.005*\"indonesia\" + 0.005*\"product\" + 0.005*\"use\" + 0.005*\"service\" + 0.004*\"platform\" + 0.004*\"well\" + 0.004*\"gojek\" + 0.004*\"help\" + 0.004*\"app\" + 0.004*\"us_million\" + 0.004*\"payment\" + 0.004*\"singapore\" + 0.004*\"see\" + 0.004*\"make\" + 0.004*\"year\"\n",
      "\n",
      "8: 0.017*\"luckin\" + 0.009*\"china\" + 0.007*\"market\" + 0.006*\"use\" + 0.006*\"luckin_coffee\" + 0.005*\"product\" + 0.005*\"new\" + 0.005*\"team\" + 0.005*\"service\" + 0.005*\"user\" + 0.005*\"investigation\" + 0.004*\"meituan\" + 0.004*\"oyo\" + 0.004*\"southeast_asia\" + 0.004*\"expand\" + 0.004*\"report\" + 0.004*\"well\" + 0.004*\"deal\" + 0.003*\"platform\" + 0.003*\"one\"\n",
      "\n",
      "9: 0.009*\"us\" + 0.008*\"user\" + 0.006*\"tiktok\" + 0.005*\"service\" + 0.005*\"singapore\" + 0.005*\"new\" + 0.004*\"china\" + 0.004*\"platform\" + 0.004*\"go\" + 0.004*\"app\" + 0.004*\"need\" + 0.004*\"leflair\" + 0.004*\"use\" + 0.004*\"market\" + 0.004*\"like\" + 0.004*\"time\" + 0.004*\"would\" + 0.003*\"make\" + 0.003*\"get\" + 0.003*\"could\"\n",
      "\n",
      "10: 0.012*\"fund\" + 0.012*\"investment\" + 0.010*\"tech\" + 0.009*\"india\" + 0.007*\"firm\" + 0.007*\"new\" + 0.005*\"deal\" + 0.005*\"us_billion\" + 0.005*\"china\" + 0.005*\"softbank\" + 0.005*\"include\" + 0.005*\"raised_us_million\" + 0.005*\"us_million\" + 0.004*\"asia_read_research\" + 0.004*\"jio\" + 0.004*\"service\" + 0.004*\"help\" + 0.004*\"platform\" + 0.004*\"product\" + 0.004*\"million\"\n",
      "\n",
      "11: 0.006*\"service\" + 0.006*\"include\" + 0.006*\"platform\" + 0.005*\"synqa\" + 0.005*\"make\" + 0.005*\"market\" + 0.005*\"photo_credit\" + 0.004*\"use\" + 0.004*\"customer\" + 0.004*\"product\" + 0.004*\"see\" + 0.004*\"user\" + 0.004*\"investor\" + 0.004*\"singapore\" + 0.004*\"ceo\" + 0.004*\"would\" + 0.003*\"online\" + 0.003*\"one\" + 0.003*\"omise\" + 0.003*\"technology\"\n",
      "\n",
      "12: 0.008*\"one\" + 0.007*\"platform\" + 0.006*\"market\" + 0.005*\"service\" + 0.005*\"china\" + 0.005*\"product\" + 0.004*\"user\" + 0.004*\"new\" + 0.004*\"make\" + 0.004*\"consumer\" + 0.003*\"year\" + 0.003*\"country\" + 0.003*\"like\" + 0.003*\"use\" + 0.003*\"share\" + 0.003*\"see\" + 0.003*\"include\" + 0.003*\"however\" + 0.003*\"app\" + 0.003*\"get\"\n",
      "\n",
      "13: 0.007*\"indonesia\" + 0.004*\"like\" + 0.004*\"us_million\" + 0.004*\"investor\" + 0.004*\"fund\" + 0.004*\"market\" + 0.004*\"china\" + 0.004*\"service\" + 0.004*\"help\" + 0.004*\"platform\" + 0.004*\"would\" + 0.004*\"make\" + 0.003*\"investment\" + 0.003*\"us\" + 0.003*\"include\" + 0.003*\"go\" + 0.003*\"india\" + 0.003*\"work\" + 0.003*\"new\" + 0.003*\"well\"\n",
      "\n",
      "14: 0.010*\"grab\" + 0.008*\"platform\" + 0.007*\"service\" + 0.006*\"market\" + 0.006*\"singapore\" + 0.005*\"new\" + 0.005*\"product\" + 0.005*\"investment\" + 0.004*\"use\" + 0.004*\"offer\" + 0.004*\"firm\" + 0.004*\"help\" + 0.004*\"us\" + 0.004*\"include\" + 0.004*\"country\" + 0.004*\"photo_credit\" + 0.003*\"launch\" + 0.003*\"digital\" + 0.003*\"delivery\" + 0.003*\"one\"\n",
      "\n",
      "15: 0.012*\"singapore\" + 0.011*\"platform\" + 0.008*\"market\" + 0.007*\"service\" + 0.005*\"use\" + 0.005*\"us\" + 0.005*\"help\" + 0.004*\"provide\" + 0.004*\"technology\" + 0.004*\"firm\" + 0.004*\"new\" + 0.004*\"make\" + 0.004*\"user\" + 0.003*\"well\" + 0.003*\"online\" + 0.003*\"global\" + 0.003*\"fund\" + 0.003*\"work\" + 0.003*\"include\" + 0.003*\"solution\"\n",
      "\n",
      "16: 0.007*\"make\" + 0.005*\"launch\" + 0.005*\"use\" + 0.004*\"china\" + 0.004*\"employee\" + 0.004*\"include\" + 0.004*\"indonesia\" + 0.004*\"app\" + 0.004*\"platform\" + 0.004*\"service\" + 0.004*\"help\" + 0.004*\"get\" + 0.004*\"plan\" + 0.004*\"market\" + 0.004*\"work\" + 0.003*\"look\" + 0.003*\"one\" + 0.003*\"well\" + 0.003*\"see\" + 0.003*\"user\"\n",
      "\n",
      "17: 0.007*\"market\" + 0.005*\"product\" + 0.005*\"investment\" + 0.005*\"space\" + 0.005*\"make\" + 0.005*\"new\" + 0.005*\"include\" + 0.004*\"fund\" + 0.004*\"work\" + 0.004*\"customer\" + 0.004*\"southeast_asia\" + 0.004*\"investor\" + 0.004*\"firm\" + 0.004*\"asia\" + 0.004*\"focus\" + 0.004*\"well\" + 0.004*\"provide\" + 0.004*\"platform\" + 0.004*\"singapore\" + 0.004*\"ceo\"\n",
      "\n",
      "18: 0.013*\"tencent\" + 0.006*\"year\" + 0.005*\"people\" + 0.005*\"platform\" + 0.005*\"game\" + 0.005*\"time\" + 0.005*\"china\" + 0.005*\"service\" + 0.004*\"may\" + 0.004*\"need\" + 0.004*\"include\" + 0.004*\"market\" + 0.004*\"chinese\" + 0.004*\"use\" + 0.004*\"well\" + 0.004*\"us_billion\" + 0.004*\"product\" + 0.004*\"first\" + 0.003*\"help\" + 0.003*\"content\"\n",
      "\n",
      "19: 0.010*\"help\" + 0.008*\"data\" + 0.007*\"user\" + 0.007*\"use\" + 0.006*\"make\" + 0.006*\"need\" + 0.006*\"buzzbreak\" + 0.005*\"platform\" + 0.005*\"service\" + 0.005*\"photo_credit\" + 0.004*\"firm\" + 0.004*\"like\" + 0.004*\"app\" + 0.003*\"look\" + 0.003*\"market\" + 0.003*\"new\" + 0.003*\"growth\" + 0.003*\"product\" + 0.003*\"drive\" + 0.003*\"singapore\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0.13615821), (10, 0.25062823), (13, 0.61264855)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[corpus[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
