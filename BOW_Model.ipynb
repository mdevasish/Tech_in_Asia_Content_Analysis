{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize,word_tokenize,pos_tag\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import datetime\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import wordnet\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "import json\n",
    "np.random.seed(123456)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "my_stopwords = stopwords.words('English') + ['photo','credit','one','make','like','us','country','use']+['new','well','would',\n",
    "                                             'take','include','need','go','help','may','look']+['could','get','million','many',\n",
    "                                             'platform','however']+['provide','see','product','still','service','market','people'\n",
    "                                             ,'even']+['time','user','year','firm','add','work']+['investor','investment',\n",
    "                                             'technology','come','consumer','customer','online']+['billion','offer','good','asia'\n",
    "                                             'india','singapore','china','indonesia']+['want','find','first','start','tell',\n",
    "                                             'tech','ceo','app','big','last','number']+['another','give','two','since']+['team',\n",
    "                                             'fund','buid','founder','grow','growth','back','lead','part','become','data','data',\n",
    "                                             'allow','already','build','focus','plan','create']+['employee','share','support',\n",
    "                                             'industry','digital','financial','launch','solution','large','continue','way','mean',\n",
    "                                             'example','content'] + ['opportunity','asia','india','report','global','currently',\n",
    "                                             'world','end','month','leave','around','process','experience','accord']+['southeast',\n",
    "                                             'call','think','today','early','source','close','reach','set','according','move','across',\n",
    "                                             'reach','expect','base','point','move']+['far','access','system','city','note','right','now',\n",
    "                                            'pay','show','less','game','order','change','much','though','know','believe','late','always'\n",
    "                                            'round','player','keep','major','run','three','ecommerce','next','thing','value','statement',\n",
    "                                            'sale','local','region','partner','capital','invest']+['payment','revenue','week','operate',\n",
    "                                            'increase','raise','different']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Every day, 100k+ smart people read our newsletter.', 'You can  sign up here .', 'Hello readers, \\n Two familiar names were spotted making investments this week: Tencent and Alibaba.', '👀 \\n The intense rivalry between these Chinese giants isn’t dying down anytime soon.', 'This week, the two unknowingly brought their clash to the companies they invested in, which were both the most-funded in China this week: \\n \\n Yipin Shengxian, a Chinese fresh food discount chain supermarket, raised US$362 million in a round led by Tencent.', 'Xpeng Motors, a Chinese electric vehicle and technology company, which Alibaba invested in, raised US$300 million.', 'You can find all other important investment deals that happened in the last few days in our  weekly funding news wrap-up .', 'Let’s dive into the biggest deals and M&amp;As that recently took place.', 'The biggest deals by country \\n 🇨🇳  Yipin Shengxian : A Chinese fresh food discount chain supermarket that raised US$362 million from Capital Today, Eastern Bell Venture Capital, and Tencent Investment.', '🇯🇵  Photosynth Inc. (Akerun) : A Japanese company that creates smart-lock devices.', 'It raised US$33 million from 31Ventures, Globis Capital Partners, Japan Finance Corporation, Joyo Bank, Line Corporation, Mizuho Bank, NTT DoCoMo Ventures, Scrum Ventures, Shinsei Bank, The Norinchukin Bank, and Toppan Printing.', '🇮🇱  Silverfort : An Israeli cybersecurity company that raised US$30 million from Aspect Ventures, Citi Ventures, Maor Investments, Singtel Innov8, StageOne VC, and TLV Partners.', '🇰🇷  Skelter Labs : A South Korean AI company which raised US$13.6 million from BNK Securities, Iloom, KDB Bank, KDB Capital, Kakao Ventures, Korea Investment and Securities, Mirakle Lab, and Stonebridge Capital.', '🇮🇩  Ayopop (Ayoconnect) : An Indonesian bills payment app.', 'It raised US$5 million from AC Ventures, Amand Ventures, BRI Ventures, Brama One Ventures, Finch Capital, Kakaku.com, and Strive (formerly GREE Ventures).', 'Here’s the complete list of this week’s funding chart:  109 deals worth over US$1.6 billion this week .', 'M&amp;A \\n 🇸🇬 US-based Intuit, the parent firm of accounting software Quickbooks, has acquired Singapore-based  TradeGecko  – a software-as-a-service company that develops online inventory and order management software for small businesses – in a deal worth US$80 million.', '🇯🇵 Japanese payments firm Hey has acquired  Coubic , a local productivity software company, for an undisclosed amount.', '🇯🇵 South Korean healthcare app HealingPaper (GangnamUnni) has acquired Japanese cosmetic surgery review app  Tink (Lucmo)  for an undisclosed amount.', 'Going deeper \\n   \\n 1️⃣ A cheat sheet of tech acquisitions in Southeast Asia \\n Southeast Asia has seen its fair share of M&amp;A deals over the years, so here’s  a list  of those that have taken place, broken down by company, country, acquirer, and vertical.', 'An interesting fact from the piece: Gojek takes the lead with the most acquisitions, making eight M&amp;A deals so far – two times more than the runners-up.', '2️⃣ Insurtech: An industry to watch \\n Look out: Insurance tech could be an industry to watch.', 'In 2019, about US$7.3 billion was invested in insurtech, up from US$5.6 billion in 2018 as  reported by Crunchbase .', 'While funding for insurtech has slowed down significantly this year, with only US$2.6 billion raised from the start of the year to July 22, we still have half a year more to go.', 'One Southeast Asian insurtech startup to highlight is Singlife, which secured US$90 million from Japan’s Sumitomo Life in 2019.', 'In the same year, Singlife also managed to  triple its revenue , achieving US$171 million in total income, up from US$54 million in 2018.', '3️⃣ Are private markets the new public markets?', 'This article  explains  why public markets are losing steam.', 'Some of the reasons include companies borrowing money instead and startups raising through VC firms like SoftBank, Sequoia, and East Ventures – a route that might be potentially faster, more efficient, and cheaper than a public offering.', 'Businesses these days, which have their assets mostly parked online,  also require less capital than firms in the past, which needed  to build factories to operate.', '4️⃣ A long-term look at public to private equity in the US \\n A new   report published by Morgan Stanley  offers a ton of information into topics like the shift from public to private equities in the US.', 'It also covers issues like the drop in number of public companies compared to 25 years ago — though companies these days are much larger and older on average, and exits which used to be done via IPOs are now shifting to sales to other firms.', '“Venture capital is more cyclical than either public markets or buyouts, and recent annual investment levels have been high,” the report reads.', '5️⃣ The Hacker Way: How I taught my nephew to program \\n A  neat guide  for moms and dads on teaching your kids how to code.', 'Pitch decks for your reference \\n   \\n \\n \\n This pitch deck  helped enterprise social network Yammer raise US$17 million.', 'In 2012, the company was acquired by Microsoft.', 'Singlife’s series A pitch deck  enabled it to bank US$50 million.', 'Foursquare’s angel pitch deck  was instrumental in raising over US$1.3 million for the company.', 'Lists of most active investors in the region \\n  China  |  India  |  Indonesia  |  Japan  |  Singapore  \\n List of top-funded startups in Asia \\n  China  |  India  |  Indonesia  |  Japan  |  Hong Kong  |  Singapore  |  Vietnam  \\n \\n If you want to receive this quick analysis of our most prized content straight in your inbox everyday, then make sure you’re subscribed to our  newsletter .']\n",
      "[['Every', 'day', ',', '100k+', 'smart', 'people', 'read', 'our', 'newsletter', '.'], ['You', 'can', 'sign', 'up', 'here', '.'], ['Hello', 'readers', ',', 'Two', 'familiar', 'names', 'were', 'spotted', 'making', 'investments', 'this', 'week', ':', 'Tencent', 'and', 'Alibaba', '.']]\n",
      "[[('Every', 'DT'), ('day', 'NN'), (',', ','), ('100k+', 'CD'), ('smart', 'JJ'), ('people', 'NNS'), ('read', 'VBP'), ('our', 'PRP$'), ('newsletter', 'NN'), ('.', '.')], [('You', 'PRP'), ('can', 'MD'), ('sign', 'VB'), ('up', 'RP'), ('here', 'RB'), ('.', '.')], [('Hello', 'NNP'), ('readers', 'NNS'), (',', ','), ('Two', 'CD'), ('familiar', 'JJ'), ('names', 'NNS'), ('were', 'VBD'), ('spotted', 'VBN'), ('making', 'VBG'), ('investments', 'NNS'), ('this', 'DT'), ('week', 'NN'), (':', ':'), ('Tencent', 'NN'), ('and', 'CC'), ('Alibaba', 'NNP'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "df['sentences'] = df['content'].apply(sent_tokenize)\n",
    "print(df['sentences'].head(1).tolist()[0])\n",
    "\n",
    "df['tokens_sentences'] = df['sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "print(df['tokens_sentences'].head(1).tolist()[0][:3])\n",
    "\n",
    "df['POS_tokens'] = df['tokens_sentences'].apply(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "print(df['POS_tokens'].head(1).tolist()[0][:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Every', 'day', ',', '100k+', 'smart', 'people', 'read', 'our', 'newsletter', '.'], ['You', 'can', 'sign', 'up', 'here', '.'], ['Hello', 'reader', ',', 'Two', 'familiar', 'name', 'be', 'spot', 'make', 'investment', 'this', 'week', ':', 'Tencent', 'and', 'Alibaba', '.']]\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "df['tokens_sentences_lemmatized'] = df['POS_tokens'].apply(lambda list_POS: \n",
    "[\n",
    "[lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS] \n",
    "        for tokens_POS in list_POS\n",
    "    ]\n",
    ")\n",
    "print(df['tokens_sentences_lemmatized'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "df['tokens'] = df['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df['tokens'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = gensim.corpora.Dictionary(tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-15 13:52:41.400399\n",
      "#Topics: 2 Coherence Score:  0.20059169943947475 Perplexity score : -8.830720249594936\n",
      "#Topics: 3 Coherence Score:  0.21872820264844464 Perplexity score : -9.045711622255173\n",
      "#Topics: 4 Coherence Score:  0.22188021101922856 Perplexity score : -9.238430665288483\n",
      "#Topics: 5 Coherence Score:  0.21490472746328168 Perplexity score : -9.41221460050993\n",
      "#Topics: 6 Coherence Score:  0.23375555998115238 Perplexity score : -9.594129966169453\n",
      "#Topics: 7 Coherence Score:  0.2287599955812219 Perplexity score : -9.72495003646242\n",
      "#Topics: 8 Coherence Score:  0.22405631353864575 Perplexity score : -9.87999074100177\n",
      "#Topics: 9 Coherence Score:  0.23445050254268407 Perplexity score : -10.029710784226857\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "    \n",
    "#model_list = []\n",
    "coherence_values = []\n",
    "perplexity_values = []\n",
    "model_topics = []\n",
    "\n",
    "for num_topics in range(2,10):\n",
    "    lda_x = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "    coherencemodel = CoherenceModel(model=lda_x, texts=tokens, dictionary=dictionary_LDA, coherence='c_v')\n",
    "    model_topics.append(num_topics)\n",
    "    #model_list.append(lda_x)\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    perplexity_values.append(lda_x.log_perplexity(corpus))\n",
    "    print(\"#Topics: \" + str(num_topics) + \" Coherence Score: \" \n",
    "              , str(coherencemodel.get_coherence())+ ' Perplexity score : '+ str(lda_x.log_perplexity(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "0.003*\"bank\" + 0.003*\"aim\" + 0.002*\"brand\" + 0.002*\"sell\" + 0.002*\"develop\" + 0.002*\"deal\" + 0.002*\"expand\" + 0.002*\"strategy\" + 0.002*\"claim\" + 0.002*\"gojek\" + 0.002*\"transaction\" + 0.002*\"funding\" + 0.002*\"bring\" + 0.002*\"pandemic\" + 0.002*\"explain\" + 0.002*\"group\" + 0.002*\"announce\" + 0.002*\"scale\" + 0.002*\"money\" + 0.002*\"round\"\n",
      "\n",
      "1:\n",
      "0.003*\"pandemic\" + 0.003*\"cost\" + 0.002*\"expand\" + 0.002*\"network\" + 0.002*\"food_delivery\" + 0.002*\"brand\" + 0.002*\"delivery\" + 0.002*\"economy\" + 0.002*\"grab\" + 0.002*\"feature\" + 0.002*\"space\" + 0.002*\"gojek\" + 0.002*\"government\" + 0.002*\"home\" + 0.002*\"merchant\" + 0.002*\"aim\" + 0.002*\"shopee\" + 0.002*\"group\" + 0.002*\"operation\" + 0.002*\"enterprise\"\n",
      "\n",
      "2:\n",
      "0.004*\"grab\" + 0.003*\"chinese\" + 0.003*\"deal\" + 0.003*\"tiktok\" + 0.002*\"delivery\" + 0.002*\"gojek\" + 0.002*\"develop\" + 0.002*\"oyo\" + 0.002*\"round\" + 0.002*\"space\" + 0.002*\"serve\" + 0.002*\"group\" + 0.002*\"apps\" + 0.002*\"brand\" + 0.002*\"pandemic\" + 0.002*\"expand\" + 0.002*\"announce\" + 0.002*\"operation\" + 0.002*\"management\" + 0.002*\"program\"\n",
      "\n",
      "3:\n",
      "0.004*\"grab\" + 0.002*\"space\" + 0.002*\"deal\" + 0.002*\"fintech\" + 0.002*\"money\" + 0.002*\"develop\" + 0.002*\"claim\" + 0.002*\"logistics\" + 0.002*\"recently\" + 0.002*\"operation\" + 0.002*\"place\" + 0.002*\"expand\" + 0.002*\"scale\" + 0.002*\"merchant\" + 0.002*\"home\" + 0.002*\"account\" + 0.002*\"aim\" + 0.002*\"feature\" + 0.002*\"future\" + 0.002*\"exist\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_topics = 4\n",
    "lda_x = models.LdaModel(corpus, num_topics=num_topics, \n",
    "                                  id2word=dictionary_LDA,\n",
    "                                  alpha=[0.01]*num_topics,\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "for i,topic in lda_x.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(str(i)+\":\\n\"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible Topics from the Model\n",
    "\n",
    "0:\n",
    "0.003*\"bank\" + 0.003*\"aim\" + 0.002*\"brand\" + 0.002*\"sell\" + 0.002*\"develop\" + 0.002*\"deal\" + 0.002*\"expand\" + 0.002*\"strategy\" + 0.002*\"claim\" + 0.002*\"gojek\" + 0.002*\"transaction\" + 0.002*\"funding\" + 0.002*\"bring\" + 0.002*\"pandemic\" + 0.002*\"explain\" + 0.002*\"group\" + 0.002*\"announce\" + 0.002*\"scale\" + 0.002*\"money\" + 0.002*\"round\"\n",
    "\n",
    "-> Topic about funding and strategy plans of Gojek \n",
    "\n",
    "1:\n",
    "0.003*\"pandemic\" + 0.003*\"cost\" + 0.002*\"expand\" + 0.002*\"network\" + 0.002*\"food_delivery\" + 0.002*\"brand\" + 0.002*\"delivery\" + 0.002*\"economy\" + 0.002*\"grab\" + 0.002*\"feature\" + 0.002*\"space\" + 0.002*\"gojek\" + 0.002*\"government\" + 0.002*\"home\" + 0.002*\"merchant\" + 0.002*\"aim\" + 0.002*\"shopee\" + 0.002*\"group\" + 0.002*\"operation\" + 0.002*\"enterprise\"\n",
    "\n",
    "-> Topic about food and goods delivery during pandemic by Grab, Gojek and Shopee \n",
    "\n",
    "2:\n",
    "0.004*\"grab\" + 0.003*\"chinese\" + 0.003*\"deal\" + 0.003*\"tiktok\" + 0.002*\"delivery\" + 0.002*\"gojek\" + 0.002*\"develop\" + 0.002*\"oyo\" + 0.002*\"round\" + 0.002*\"space\" + 0.002*\"serve\" + 0.002*\"group\" + 0.002*\"apps\" + 0.002*\"brand\" + 0.002*\"pandemic\" + 0.002*\"expand\" + 0.002*\"announce\" + 0.002*\"operation\" + 0.002*\"management\" + 0.002*\"program\"\n",
    "\n",
    "-> Unclear topic about Grab, Chinese Tiktok and Gojek\n",
    "\n",
    "3:\n",
    "0.004*\"grab\" + 0.002*\"space\" + 0.002*\"deal\" + 0.002*\"fintech\" + 0.002*\"money\" + 0.002*\"develop\" + 0.002*\"claim\" + 0.002*\"logistics\" + 0.002*\"recently\" + 0.002*\"operation\" + 0.002*\"place\" + 0.002*\"expand\" + 0.002*\"scale\" + 0.002*\"merchant\" + 0.002*\"home\" + 0.002*\"account\" + 0.002*\"aim\" + 0.002*\"feature\" + 0.002*\"future\" + 0.002*\"exist\"\n",
    "\n",
    "-> Topic about expansion plans of Grab into Fintech space by acquiring Banking liscence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(model,vecs,dictionary):\n",
    "    visual= pyLDAvis.gensim.prepare(model, vecs, dictionary)\n",
    "    pyLDAvis.save_html(visual, \"./Models/BOW/my_topics_bow.html\")\n",
    "    return True\n",
    "\n",
    "x = visualise(model = lda_x, vecs = corpus,dictionary = dictionary_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_x.save('./Models/BOW/Topicmodelling_bow.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.999659)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_x[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mix = []\n",
    "for i in range(900):\n",
    "    #print('Doc :',i,' Topic',lda_x[corpus[i]])\n",
    "    mix.append(lda_x[corpus[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Topic_Mixture'] = mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = dict(dictionary_LDA)\n",
    "json.dump( dct, open( \"./Models/BOW/dictionary_lda.json\", 'w' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bow = df[['id', 'title', 'content', 'excerpt', 'comments_count', 'read_time',\n",
    "       'author.display_name','tokens','Topic_Mixture']]\n",
    "final_bow.to_csv('./Models/BOW/BOW_Topic_Modelling.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
